{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPjDNOiBAQ0NqXrzwcPMVTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raeubaen/ml/blob/master/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGtq2ye_vC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import cupy as cp\n",
        "import cupy.cuda.curand as curand\n",
        "from mpi4py import MPI\n",
        "from numba import cuda\n",
        "from numba import vectorize\n",
        "import numpy as np\n",
        "\n",
        "# Setup MPI and get neighbor ranks\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.rank\n",
        "rank_up = comm.rank - 1 if (comm.rank - 1 >= 0) else comm.size - 1\n",
        "rank_down = comm.rank + 1 if (comm.rank + 1 < comm.size) else 0\n",
        "# Set device\n",
        "cuda.select_device(rank)\n",
        "\n",
        "\n",
        "class Args:\n",
        "  def __init__(self, **kwargs):\n",
        "    for key in kwargs:\n",
        "      setattr(self, key, kwargs[key])\n",
        "\n",
        "args = Args(\n",
        "    lattice_n=28,\n",
        "    lattice_m=28,\n",
        "    nwarmup=100,\n",
        "    niters=100,\n",
        "    write_lattice=True,\n",
        "    use_common_seed=True,\n",
        "    seed=1234,\n",
        ")\n",
        "\n",
        "'''\n",
        "# Parse command line arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--lattice-n\", '-x', type=int, default=40*128, help=\"number of lattice rows\")\n",
        "parser.add_argument(\"--lattice-m\", '-y', type=int, default=40*128, help=\"number of lattice columns\")\n",
        "parser.add_argument(\"--nwarmup\", '-w', type=int, default=100, help=\"number of warmup iterations\")\n",
        "parser.add_argument(\"--niters\", '-n', type=int, default=1000, help=\"number of trial iterations\")\n",
        "parser.add_argument(\"--alpha\", '-a', type=float, default=0.1, help=\"coefficient of critical temperature\")\n",
        "parser.add_argument(\"--seed\", '-s', type=int, default=1234, help=\"seed for random number generation\")\n",
        "parser.add_argument(\"--write-lattice\", '-o', action='store_true', help=\"write final lattice configuration to file/s\")\n",
        "parser.add_argument(\"--use-common-seed\", '-c', action='store_true', help=\"Use common seed for all ranks + updating offset. \" +\n",
        "                                                                         \"Yields consistent results independent of number \" +\n",
        "                                                                         \"of GPUs but is slower.\")\n",
        "args = parser.parse_args()\n",
        "'''\n",
        "# Check arguments\n",
        "if args.lattice_m % 2 != 0:\n",
        "    raise Exception(\"lattice_m must be an even value. Aborting.\")\n",
        "if args.lattice_n % comm.size != 0:\n",
        "    raise Exception(\"lattice_n must be evenly divisible by number of GPUs. Aborting.\")\n",
        "if (args.lattice_n / comm.size) % 2 != 0:\n",
        "    raise Exception(\"Slab width (lattice_n / nGPUs) must be an even value. Aborting.\")\n",
        "\n",
        "# Compute slab width\n",
        "lattice_slab_n = args.lattice_n // comm.size\n",
        "\n",
        "# Generate lattice with random spins with shape of randval array\n",
        "@vectorize(['int8(float32)'], target='cuda')                             \n",
        "def generate_lattice(randval):\n",
        "    return 1 if randval > 0.5 else -1 \n",
        "\n",
        "@cuda.jit\n",
        "def update_lattice_multi(lattice, op_lattice, op_lattice_up, op_lattice_down, randvals, is_black, inv_temp):\n",
        "    n,m = lattice.shape\n",
        "    tid = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    j = tid % m\n",
        "    i = tid // m\n",
        "\n",
        "    if (i >= n or j >= m): return\n",
        "\n",
        "    # Set stencil indices with periodicity\n",
        "    jpp = (j + 1) if (j + 1) < m else 0\n",
        "    jnn = (j - 1) if (j - 1) >= 0 else (m - 1)\n",
        "\n",
        "    # Select off-column index based on color and row index parity\n",
        "    if (is_black):\n",
        "        joff = jpp if (i % 2) else jnn\n",
        "    else:\n",
        "        joff = jnn if (i % 2) else jpp\n",
        "\n",
        "    # Compute sum of nearest neighbor spins (taking values from neighboring\n",
        "    # lattice slabs if required)\n",
        "    nn_sum = op_lattice[i, j] + op_lattice[i, joff]\n",
        "    nn_sum += op_lattice[i - 1, j] if (i - 1) >= 0 else op_lattice_up[n - 1, j]\n",
        "    nn_sum += op_lattice[i + 1, j] if (i + 1) < n else op_lattice_down[0, j]\n",
        "\n",
        "    # Determine whether to flip spin\n",
        "    lij = lattice[i, j]\n",
        "    acceptance_ratio = math.exp(-2.0 * inv_temp * nn_sum * lij)\n",
        "    if (randvals[i, j] < acceptance_ratio):\n",
        "        lattice[i, j] = -lij\n",
        "\n",
        "# Create lattice update kernel (for single GPU case, this version with fewer arguments\n",
        "# is a bit faster due to launch overhead introduced by numba)\n",
        "@cuda.jit\n",
        "def update_lattice(lattice, op_lattice, randvals, is_black, inv_temp):\n",
        "    n,m = lattice.shape\n",
        "    tid = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    i = tid // m\n",
        "    j = tid % m\n",
        "\n",
        "    if (i >= n or j >= m): return\n",
        "\n",
        "    # Set stencil indices with periodicity\n",
        "    ipp = (i + 1) if (i + 1) < n else 0\n",
        "    jpp = (j + 1) if (j + 1) < m else 0\n",
        "    inn = (i - 1) if (i - 1) >= 0 else (n - 1)\n",
        "    jnn = (j - 1) if (j - 1) >= 0 else (m - 1)\n",
        "\n",
        "    # Select off-column index based on color and row index parity\n",
        "    if (is_black):\n",
        "        joff = jpp if (i % 2) else jnn\n",
        "    else:\n",
        "        joff = jnn if (i % 2) else jpp\n",
        "\n",
        "    # Compute sum of nearest neighbor spins\n",
        "    nn_sum = op_lattice[inn, j] + op_lattice[i, j] + op_lattice[ipp, j] + op_lattice[i, joff]\n",
        "\n",
        "    # Determine whether to flip spin\n",
        "    lij = lattice[i, j]\n",
        "    acceptance_ratio = math.exp(-2.0 * inv_temp * nn_sum * lij)\n",
        "    if (randvals[i, j] < acceptance_ratio):\n",
        "        lattice[i, j] = -lij\n",
        "\n",
        "# Write lattice configuration to file\n",
        "def write_lattice(prefix, lattice_b, lattice_w):\n",
        "  lattice_b_h = lattice_b.copy_to_host()\n",
        "  lattice_w_h = lattice_w.copy_to_host()\n",
        "  lattice = np.zeros((lattice_slab_n, args.lattice_m), dtype=np.int8)\n",
        "  for i in range(lattice_slab_n):\n",
        "      for j in range(args.lattice_m // 2):\n",
        "          if (i % 2):\n",
        "              lattice[i, 2*j+1] = lattice_b_h[i, j]\n",
        "              lattice[i, 2*j] = lattice_w_h[i, j]\n",
        "          else:\n",
        "              lattice[i, 2*j] = lattice_b_h[i, j]\n",
        "              lattice[i, 2*j+1] = lattice_w_h[i, j]\n",
        "\n",
        "  #print(\"Writing lattice to {}_rank{}.txt...\".format(prefix, rank))\n",
        "  np.savetxt(\"{}_rank{}.txt\".format(prefix, rank), lattice, fmt='%d')\n",
        "\n",
        "# Helper class for random number generation\n",
        "class curandUniformRNG:\n",
        "    def __init__(self, seed=0):\n",
        "        rng = curand.createGenerator(curand.CURAND_RNG_PSEUDO_PHILOX4_32_10)\n",
        "        curand.setPseudoRandomGeneratorSeed(rng, seed)\n",
        "        if (args.use_common_seed):\n",
        "            self.offset = rank * lattice_slab_n * args.lattice_m // 2\n",
        "            curand.setGeneratorOffset(rng, self.offset)\n",
        "        self._rng = rng\n",
        "\n",
        "    def fill_random(self, arr):\n",
        "        ptr = arr.__cuda_array_interface__['data'][0]\n",
        "        curand.generateUniform(self._rng, ptr, arr.size)\n",
        "        if (args.use_common_seed):\n",
        "            self.offset += args.lattice_n * args.lattice_m // 2\n",
        "            curand.setGeneratorOffset(self._rng, self.offset)\n",
        "\n",
        "# Helper function to perform device sync plus MPI barrier\n",
        "def sync():\n",
        "  cuda.synchronize()\n",
        "  comm.barrier()\n",
        "\n",
        "def update(lattices_b, lattices_w, randvals, rng, inv_temp):\n",
        "    # Setup CUDA launch configuration\n",
        "    threads = 128\n",
        "    blocks = (args.lattice_m // 2 * lattice_slab_n + threads - 1) // threads\n",
        "\n",
        "    if (comm.size > 1):\n",
        "        # Update black\n",
        "        rng.fill_random(randvals)\n",
        "        update_lattice_multi[blocks, threads](lattices_b[rank], lattices_w[rank], lattices_w[rank_up], lattices_w[rank_down], randvals, True, inv_temp)\n",
        "        sync()\n",
        "        # Update white\n",
        "        rng.fill_random(randvals)\n",
        "        update_lattice_multi[blocks, threads](lattices_w[rank], lattices_b[rank], lattices_b[rank_up], lattices_b[rank_down], randvals, False, inv_temp)\n",
        "        sync()\n",
        "    else:\n",
        "        # Update black\n",
        "        rng.fill_random(randvals)\n",
        "        update_lattice[blocks, threads](lattices_b[rank], lattices_w[rank], randvals, True, inv_temp)\n",
        "        # Update white\n",
        "        rng.fill_random(randvals)\n",
        "        update_lattice[blocks, threads](lattices_w[rank], lattices_b[rank], randvals, False, inv_temp)\n",
        "\n",
        "def run(n, inv_temp):\n",
        "  cp.get_default_memory_pool().free_all_blocks()\n",
        "\n",
        "  # Setup cuRAND generator\n",
        "  rng = curandUniformRNG(seed=args.seed if args.use_common_seed else args.seed + 42 * rank)\n",
        "  randvals = cuda.device_array((lattice_slab_n, args.lattice_m // 2), dtype=np.float32)\n",
        "\n",
        "  # Setup black and white lattice arrays on device\n",
        "  rng.fill_random(randvals)\n",
        "  lattice_b = generate_lattice(randvals)\n",
        "  rng.fill_random(randvals)\n",
        "  lattice_w = generate_lattice(randvals)\n",
        "\n",
        "  # Setup/open CUDA IPC handles\n",
        "  ipch_b = comm.allgather(lattice_b.get_ipc_handle())\n",
        "  ipch_w = comm.allgather(lattice_w.get_ipc_handle())\n",
        "  lattices_b = [x.open() if i != rank else lattice_b for i,x in enumerate(ipch_b)]\n",
        "  lattices_w = [x.open() if i != rank else lattice_w for i,x in enumerate(ipch_w)]\n",
        "\n",
        "  # Warmup iterations\n",
        "  if rank == 0:\n",
        "      #print(\"Starting warmup...\")\n",
        "      #sys.stdout.flush()\n",
        "      pass\n",
        "  sync()\n",
        "  for i in range(args.nwarmup):\n",
        "      update(lattices_b, lattices_w, randvals, rng, inv_temp)\n",
        "  sync()\n",
        "\n",
        "  # Trial iterations\n",
        "  if rank == 0:\n",
        "      #print(\"Starting trial iterations...\")\n",
        "      #sys.stdout.flush()\n",
        "      pass\n",
        "  t0 = time.time()\n",
        "  for i in range(args.niters):\n",
        "      update(lattices_b, lattices_w, randvals, rng, inv_temp)\n",
        "      if (rank == 0 and i % 1000 == 0):\n",
        "          #print(\"Completed {}/{} iterations...\".format(i+1, args.niters))\n",
        "          #sys.stdout.flush()\n",
        "          pass\n",
        "  sync()\n",
        "\n",
        "  t1 = time.time()\n",
        "  t = t1 - t0\n",
        "\n",
        "  # Compute average magnetism\n",
        "  m = (np.sum(lattices_b[rank], dtype=np.int64) + np.sum(lattices_w[rank], dtype=np.int64)) / float(args.lattice_n * args.lattice_m)\n",
        "  m_global = comm.allreduce(m, MPI.SUM)\n",
        "\n",
        "  '''\n",
        "  if (rank == 0):\n",
        "    print(\"REPORT:\")\n",
        "    print(\"\\tnGPUs: {}\".format(comm.size))\n",
        "    print(\"\\ttemperature: {}\".format(1/inv_temp))\n",
        "    print(\"\\tseed: {}\".format(args.seed))\n",
        "    print(\"\\twarmup iterations: {}\".format(args.nwarmup))\n",
        "    print(\"\\ttrial iterations: {}\".format(args.niters))\n",
        "    print(\"\\tlattice dimensions: {} x {}\".format(args.lattice_n, args.lattice_m))\n",
        "    print(\"\\telapsed time: {} sec\".format(t))\n",
        "    print(\"\\tupdates per ns: {}\".format((args.lattice_n * args.lattice_m * args.niters) / t * 1e-9))\n",
        "    print(\"\\taverage magnetism (absolute): {}\".format(np.abs(m_global)))\n",
        "    sys.stdout.flush()\n",
        "  '''\n",
        "  sync()\n",
        "\n",
        "  write_lattice(f\"final{n}\", lattices_b[rank], lattices_w[rank])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj9SuoFcEQxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "07679042-dcd2-4f28-8465-22c36c8fdb98"
      },
      "source": [
        "N = 100000\n",
        "\n",
        "for i in range(1, N):\n",
        "  print(f\"\\r{i}\", end=\"\")\n",
        "  temp = 5/N*i\n",
        "  run(i, 1/temp)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2843"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CURANDError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCURANDError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-296ae808b2ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\r{i}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-f23c1c2ba7ed>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(n, inv_temp)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;31m# Setup black and white lattice arrays on device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m   \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m   \u001b[0mlattice_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_lattice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-f23c1c2ba7ed>\u001b[0m in \u001b[0;36mfill_random\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfill_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cuda_array_interface__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mcurand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerateUniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_common_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlattice_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlattice_m\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcupy/cuda/curand.pyx\u001b[0m in \u001b[0;36mcupy.cuda.curand.generateUniform\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/cuda/curand.pyx\u001b[0m in \u001b[0;36mcupy.cuda.curand.generateUniform\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcupy/cuda/curand.pyx\u001b[0m in \u001b[0;36mcupy.cuda.curand.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mCURANDError\u001b[0m: CURAND_STATUS_ALLOCATION_FAILED"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G26T7PfhC1Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "img = np.loadtxt(\"final0_rank0.txt\")\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
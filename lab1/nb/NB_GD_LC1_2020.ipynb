{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/raeubaen/giagu/blob/master/NB_GD_LC1_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tecniche di Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Informazioni:\n",
        "\n",
        "scopo: fare pratica con l'uso della tecnica di discesa lungo il gradiente e delle tecniche associate (momentum, ADAM, RMSProp, ...) per lo studio dei minimi di funzioni bidimensionali."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Prima parte: visualizzazione grafica delle funzioni**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# setup delle funzioni di visualizzazione di base usate successivamente\n",
        "\n",
        "# configura matplotlib in modo che l'output sia in linea con il codice della cella del notebook che lo ha\n",
        "# prodotto e sia salvato con il notebook stesso\n",
        "%matplotlib inline\n",
        "\n",
        "# importa le librerie necessarie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# 3D plots\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import LogNorm\n",
        "from collections import OrderedDict as OrD\n",
        "\n",
        "# funzioni per plottare\n",
        "def plot_surface(x, y, z,\n",
        "    azim=-60, elev=40, dist=10, cmap=\"RdYlBu_r\",\n",
        "    xlimL=1, xlimH=1, ylimL=1, ylimH=1, zlimL=2, zlimH=2,):\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    plot_args = {\n",
        "        \"rstride\": 1,\n",
        "        \"cstride\": 1,\n",
        "        \"cmap\": cmap,\n",
        "        \"linewidth\": 20,\n",
        "        \"antialiased\": True,\n",
        "        \"vmin\": -2,\n",
        "        \"vmax\": 2,\n",
        "    }\n",
        "    ax.plot_surface(x, y, z, **plot_args)\n",
        "    ax.view_init(azim=azim, elev=elev)\n",
        "    ax.dist = dist\n",
        "    ax.set_xlim(-xlimL, xlimH)\n",
        "    ax.set_ylim(-ylimL, ylimH)\n",
        "    ax.set_zlim(-zlimL, zlimH)\n",
        "\n",
        "    plt.xticks([-1, -0.5, 0, 0.5, 1], [\"-1\", \"-1/2\", \"0\", \"1/2\", \"1\"])\n",
        "    plt.yticks([-1, -0.5, 0, 0.5, 1], [\"-1\", \"-1/2\", \"0\", \"1/2\", \"1\"])\n",
        "    ax.set_zticks([-2, -1, 0, 1, 2])\n",
        "    ax.set_zticklabels([\"-2\", \"-1\", \"0\", \"1\", \"2\"])\n",
        "\n",
        "    ax.set_xlabel(\"x\", fontsize=18)\n",
        "    ax.set_ylabel(\"y\", fontsize=18)\n",
        "    ax.set_zlabel(\"z\", fontsize=18)\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def overlay_trajectory_quiver(ax, obj_func, trajectory, color=\"k\"):\n",
        "    xs = trajectory[:, 0]\n",
        "    ys = trajectory[:, 1]\n",
        "    zs = obj_func(xs, ys)\n",
        "    ax.quiver(\n",
        "        xs[:-1],\n",
        "        ys[:-1],\n",
        "        zs[:-1],\n",
        "        xs[1:] - xs[:-1],\n",
        "        ys[1:] - ys[:-1],\n",
        "        zs[1:] - zs[:-1],\n",
        "        color=color,\n",
        "        arrow_length_ratio=0.3,\n",
        "    )\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def overlay_trajectory(ax, obj_func, trajectory, label, color=\"k\"):\n",
        "    xs = trajectory[:, 0]\n",
        "    ys = trajectory[:, 1]\n",
        "    zs = obj_func(xs, ys)\n",
        "    ax.plot(xs, ys, zs, color, label=label)\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def overlay_trajectory_contour_M(ax, trajectory, label, color=\"k\", lw=2):\n",
        "    xs = trajectory[:, 0]\n",
        "    ys = trajectory[:, 1]\n",
        "    ax.plot(xs, ys, color, label=label, lw=lw)\n",
        "    ax.plot(xs[-1], ys[-1], color + \">\", markersize=14)\n",
        "    return ax\n",
        "\n",
        "\n",
        "def overlay_trajectory_contour(ax, trajectory, label, color=\"k\", lw=2):\n",
        "    xs = trajectory[:, 0]\n",
        "    ys = trajectory[:, 1]\n",
        "    ax.plot(xs, ys, color, label=label, lw=lw)\n",
        "    return ax"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Funzioni da studiare**\n",
        "\n",
        "* funzione con minimo quadratico: $$z=ax^2+by^2,$$\n",
        "\n",
        "* fuzione con un punto di sella: $$z=ax^2-by^2,$$\n",
        "\n",
        "* funzione convessa di Beale: $$z = (1.5-x+xy)^2+(2.25-x+xy^2)^2+(2.625-x+xy^3)^2$$ con minimo in $$(\\hat{x},\\hat{y})=(3,0.5)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# funzione con minimo quadratico\n",
        "def minima_surface(x, y, a=1, b=1):\n",
        "    return a * x ** 2 + b * y ** 2 - 1\n",
        "\n",
        "\n",
        "# gradiente analitico\n",
        "def grad_minima_surface(params, a=1, b=1):\n",
        "    x = params[0]\n",
        "    y = params[1]\n",
        "    grad_x = 2 * a * x\n",
        "    grad_y = 2 * b * y\n",
        "    return [grad_x, grad_y]\n",
        "\n",
        "\n",
        "# funzione con punto di sella\n",
        "def saddle_surface(x, y, a=1, b=1):\n",
        "    return a * x ** 2 - b * y ** 2\n",
        "\n",
        "\n",
        "# gradiente analitico\n",
        "def grad_saddle_surface(params, a=1, b=1):\n",
        "    x = params[0]\n",
        "    y = params[1]\n",
        "    grad_x = 2 * a * x\n",
        "    grad_y = -2 * b * y\n",
        "    return [grad_x, grad_y]\n",
        "\n",
        "\n",
        "# funzione di Beale\n",
        "def beales_function(x, y):\n",
        "    z = (\n",
        "        np.square(1.5 - x + x * y)\n",
        "        + np.square(2.25 - x + x * y * y)\n",
        "        + np.square(2.625 - x + x * y ** 3)\n",
        "    )\n",
        "    return z\n",
        "\n",
        "\n",
        "# gradiente analitico\n",
        "def grad_beales_function(params):\n",
        "    x = params[0]\n",
        "    y = params[1]\n",
        "    grad_x = (\n",
        "        2 * (1.5 - x + x * y) * (-1 + y)\n",
        "        + 2 * (2.25 - x + x * y ** 2) * (-1 + y ** 2)\n",
        "        + 2 * (2.625 - x + x * y ** 3) * (-1 + y ** 3)\n",
        "    )\n",
        "    grad_y = (\n",
        "        2 * (1.5 - x + x * y) * x\n",
        "        + 4 * (2.25 - x + x * y ** 2) * x * y\n",
        "        + 6 * (2.625 - x + x * y ** 3) * x * y ** 2\n",
        "    )\n",
        "    return [grad_x, grad_y]\n",
        "\n",
        "\n",
        "# contour plot della funzione di Beale\n",
        "def contour_beales_function():\n",
        "    # plot beales function\n",
        "    x, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    z = beales_function(x, y)\n",
        "    cax = ax.contour(\n",
        "        x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\"\n",
        "    )\n",
        "    ax.plot(3, 0.5, \"r*\", markersize=18)\n",
        "    ax.set_xlabel(\"$x$\")\n",
        "    ax.set_ylabel(\"$y$\")\n",
        "    ax.set_xlim((-4.5, 4.5))\n",
        "    ax.set_ylim((-4.5, 4.5))\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "# plot delle superfici\n",
        "plt.close()  # chiudi qualsiasi plot precedente\n",
        "x, y = np.mgrid[-1:1:31j, -1:1:31j]\n",
        "fig1, ax1 = plot_surface(x, y, minima_surface(x, y, 1), 30)\n",
        "fig2, ax2 = plot_surface(x, y, saddle_surface(x, y))\n",
        "x, y = np.mgrid[-2:2:31j, -2:2:31j]\n",
        "fig3, ax3 = plot_surface(x, y, beales_function(x, y),\n",
        "    xlimL=2.1, xlimH=2.1, ylimL=2.1, ylimH=2.1, zlimL=0, zlimH=300,)\n",
        "\n",
        "# Contour plot della funzione di Bale\n",
        "fig4, ax4 = contour_beales_function()\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 2: discesa lungo il gradiente con e senza momentum\n",
        "\n",
        "<b>simple Gradient Descent</b>\n",
        "\n",
        "Implementazione di una semplice discesa lungo il gradiente, in cui dato un set di parametri $w$, qusti vengono aggiornati ad ogni iterazione nella direzione del gradiente locale:\n",
        "\n",
        "$$w_{t+1}= w_t - \\eta_t \\nabla_w E(w),$$\n",
        "\n",
        "in cui $E(w)$ rappresenta la funzione (energia nel linguaggio della fisica) che vogliamo minimizzare.\n",
        "\n",
        "Il parametro $\\eta_t$ (lerning rate) controlla la larghezza di ciascuno step. In generale l'algoritmo \u00e8 estremamente sensibile al valore di $\\eta_t$, se \u00e8 troppo grande possono comparire grandi oscillazioni intorno al minimo (perdendo controllo sule strutture a piccola scala della funzione). Il problema diventa sempre pi\u00f9 grande quanto pi\u00f9 rumore \u00e8 presente nelle stime de gradiente (come succede nel ML in cui il gradiente viene cacolato su campioni di eventi di dimensione limitata). Se $\\eta_t$ \u00e8 troppo piccola la porcedura di ottimizzazione diviene estremamente lenta.\n",
        "\n",
        "\n",
        "<b>Gradient Descent + Momentum</b>\n",
        "\n",
        "Aggiungere informazione su cosa \u00e8 successo nelle iterazioni precedenti (\"da dove proviene la palla che rotola gi\u00f9 dalla collina\") aiuta ad evitare problemi con minimi locali sopratutto in presenza di rumore stocastico.\n",
        "Questo pu\u00f2 essere fatto aggiungendo un termine di memoria o momentum che permette sia di risucire ad uscire da minimi locali, sia di non reagire in modo esagerato ai cambiamenti presenti ad ogni nuova iterazione.\n",
        "La regola di update diviene:\n",
        "\n",
        "$$\n",
        "v_{t}=\\gamma v_{t-1}+\\eta_{t}\\nabla_w E(w_t),\\\\\n",
        "w_{t+1}= w_t -v_{t},\n",
        "$$\n",
        "\n",
        "in cui $0\\le \\gamma < 1$ \u00e8 un parametro detto momentum. Quando $\\gamma=0$ si riottiene il GD ordinario, aumentando \n",
        "$\\gamma$ aumenta il contributo di inerzia al gardiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# implementazione funzioni GD e GD+momentum\n",
        "# nota viene introdotta la possibilit\u00e0 di aggiungere una componente rumorosa al calcolo del gradiente per simulare\n",
        "# l'effetto nell'uso del ML con campioni di dimensione finita\n",
        "\n",
        "'''\n",
        "Arguments:\n",
        "  defined: Dict - contains the hyperparameters values needed (with laTex keys, to copy it in the legend)\n",
        "  default: Dict (if only one key) or OrderedDict - contains the default hyperparameters values\n",
        "Returns a list of hyperparameters values as in 'defined',\n",
        "with the same order of 'default'.\n",
        "If an hyperparameter is not in 'defined', its value is taken from default.\n",
        "'''\n",
        "def get_par(defined, default):\n",
        "    return [defined[key] if key in defined else default[key] for key in default]\n",
        "\n",
        "# GD {}\n",
        "def gd(grad, init, hyper_par, n_epochs=1000, noise_strength=0):\n",
        "    default_par = {\"$\\eta$\": 0.1}\n",
        "    eta = get_par(hyper_par, default_par)\n",
        "\n",
        "    params = np.array(init)\n",
        "    param_traj = np.zeros([n_epochs + 1, 2])\n",
        "    param_traj[0,] = init\n",
        "    v = 0\n",
        "    for j in range(n_epochs):\n",
        "        noise = noise_strength * np.random.randn(params.size)\n",
        "        v = eta * (np.array(grad(params)) + noise)\n",
        "        params = params - v\n",
        "        param_traj[j + 1,] = params\n",
        "    legend = \"GD \" + \", \".join(f\"{key}={hyper_par[key]}\" for key in hyper_par)\n",
        "    return (param_traj, legend)\n",
        "\n",
        "\n",
        "# GD + momentum\n",
        "def gd_with_mom(grad, init, hyper_par, n_epochs=5000, noise_strength=0):\n",
        "    default_par = OrD([(\"$\\eta$\", 0.1), (\"$\\gamma$\", 0.9)])\n",
        "    eta, gamma = get_par(hyper_par, default_par)\n",
        "\n",
        "    params = np.array(init)\n",
        "    param_traj = np.zeros([n_epochs + 1, 2])\n",
        "    param_traj[0,] = init\n",
        "    v = 0\n",
        "    for j in range(n_epochs):\n",
        "        noise = noise_strength * np.random.randn(params.size)\n",
        "        v = gamma * v + eta * (np.array(grad(params)) + noise)\n",
        "        params = params - v\n",
        "        param_traj[j + 1,] = params\n",
        "    legend = \"GD+M \" + \", \".join(f\"{key}={hyper_par[key]}\" for key in hyper_par)\n",
        "    return (param_traj, legend)\n",
        "\n",
        "\n",
        "def run_and_plot(ax, gradient, init, hyper_par, \n",
        "                 marker='o', n_epochs=100, opt=gd, arrow=False, noise=0):\n",
        "    param_traj, legend = opt(gradient, init, hyper_par, n_epochs=n_epochs)\n",
        "    if arrow:\n",
        "        overlay_trajectory_contour_M(ax, param_traj, legend, marker, lw=0.5)\n",
        "    else:\n",
        "        overlay_trajectory_contour(ax, param_traj, legend, marker, lw=0.5)\n",
        "    return param_traj"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Applicazione di GD e GD+momentum\n",
        "\n",
        "Applichiamo l'algoritmo alla funzione con minimo quadratico $z=ax^2+by^2-1$ con ($a=b=1$).\n",
        "\n",
        "**Cose si vuole testare:**\n",
        "* dipendenza dal learning rate: $\\eta=0.1,0.5,1,1.01$;\n",
        "* dipendenza dai parameri della funzione\n",
        "\n",
        "**Da fare come esercizio:**\n",
        "* cosa cambia modificando i parametri $a$ e $b$ della funzione? Superfici anisotrope presentano lo stesso comportamento?\n",
        "* provare cosa cambia usando GD_momentum. Come cambia la dipendenza da $\\eta$ rispetto al caso senza momentum?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# studio effetto learning rate in GD\n",
        "plt.close()\n",
        "x, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "a, b = 1.0, 1.0\n",
        "z = np.abs(minima_surface(x, y, a, b))\n",
        "ax.contour(x, y, z, levels=np.logspace(0.0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\n",
        "ax.plot(0, 0, 'r*', markersize=18)\n",
        "\n",
        "#run_and_plot(ax, gradient, init, hyper_par, marker='o', n_epochs=100, opt=gd, arrow=False, noise=0):\n",
        "\n",
        "run_and_plot(ax, grad_minima_surface, [-2,   4],   {'$\\eta$': 0.1}, 'g--*')\n",
        "run_and_plot(ax, grad_minima_surface, [-1.7, 4],   {'$\\eta$': 0.5}, 'b-<')\n",
        "run_and_plot(ax, grad_minima_surface, [-1.5, 4],   {'$\\eta$': 1}, '->')\n",
        "run_and_plot(ax, grad_minima_surface, [-3,   4.5], {'$\\eta$': 1.01}, 'c-o', n_epochs=10)\n",
        "\n",
        "plt.legend(loc=2)\n",
        "plt.show()\n",
        "fig.savefig(\"NB_GD_plot1.pdf\", bbox_inches='tight')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Svolgimento dell'esercizio\n",
        "Essendo la funzione asimmetrica compare un caso patologico, non presente in precedenza. <br>\n",
        "Il gradiente \u00e8 definito come $(2ax, 2by)$, per cui se scegliamo un punto di partenza $(x_0,y_0)$ tale che $(x_1,y_1) = (-x_0,y_1)$ allora non avremo convergenza su $x$, ma oscillazione. <br>\n",
        "Scegliamo quindi $\\eta=0.1$ (valore che in precedenza assicurava la convergenza), ma scegliamo $a = 10$, per cui: <br>\n",
        "$x_1 = x_0 - 0.1 \\cdot 2 \\cdot 10 \\cdot x_0 = - x_0$. <br>\n",
        "Sull'asse $y$ si ha invece convergenza esponenziale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#plottiamo le curve di livello\n",
        "plt.close()\n",
        "x, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax = axs[0]\n",
        "z = np.abs(minima_surface(x, y, a, b))\n",
        "ax.contour(x, y, z, levels=np.logspace(0.0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\n",
        "ax.plot(0, 0, 'r*', markersize=18)\n",
        "\n",
        "#creiamo una funzione che restituisca la funzione gradiente, fissati a e b\n",
        "def make_grad(a, b):\n",
        "    def f(params):\n",
        "        x = params[0]\n",
        "        y = params[1]\n",
        "        grad_x = 2 * a * x\n",
        "        grad_y = 2 * b * y\n",
        "        return [grad_x, grad_y]\n",
        "    return f\n",
        "\n",
        "init = [3, 4] #per migliore resa grafica\n",
        "\n",
        "ptj = run_and_plot(ax, make_grad(10, 1), init,   {'$\\eta$': 0.1}, 'g--*')\n",
        "\n",
        "ax.legend(loc=2)\n",
        "ax.grid()\n",
        "ax.set(title = \"GD con a=10, b=1\")\n",
        "\n",
        "Y = np.asarray(ptj)[:, 1]\n",
        "ax1 = axs[1]\n",
        "ax1.semilogy()\n",
        "ax1.plot(Y)\n",
        "ax1.set(ylabel=\"Valore su y\", xlabel = \"Epochs\", title=\"Convergenza su y\")\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nei casi non patologici, a differenza del caso simmetrico, la discesa non avviene lungo il raggio che va dal punto iniziale verso l'origine, ma lungo una traiettoria curva non oscillante, come si vede in figura, con $\\eta = 0.06, a=3, b=1$.\n",
        "Ci\u00f2 avviene quando $a \\cdot \\eta < 0.5$.\n",
        "Nei casi $0.5 < a \\cdot \\eta < 1$ la traiettoria, seppur convergente, presenta oscillazioni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#plottiamo le curve di livello\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "z = np.abs(minima_surface(x, y, a, b))\n",
        "for ax in axs:\n",
        "    ax.contour(x, y, z, levels=np.logspace(0.0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\n",
        "    ax.plot(0, 0, 'r*', markersize=18)\n",
        "\n",
        "ptj = run_and_plot(axs[0], make_grad(3, 1), [4,4], {'$\\eta$': 0.07}, 'g--*')\n",
        "ptj = run_and_plot(axs[1], make_grad(10, 1), [4,4], {'$\\eta$': 0.09}, 'g--*')\n",
        "\n",
        "axs[0].legend(loc=2)\n",
        "axs[1].legend(loc=2)\n",
        "\n",
        "axs[0].set(title=\"GD con a=3, b=1: $a\\cdot \\eta = 0.21$\")\n",
        "axs[1].set(title=\"GD con a=10, b=1: $a\\cdot \\eta = 0.9$\")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tecniche GD con uso di learning rate adattivi: RMSProp e ADAM\n",
        "\n",
        "Il learning rate \u00e8 fissato dalla direzione pi\u00f9 ripida del landscape della funzione da minimizzare, ma questa pu\u00f2 cambiare a seconda del punto locale in cui ci troviamo. <br>\n",
        "Per aggirare questo problema, idealmente dovremmo scegliere di dare grandi step nelle direzioni che corrispondono a zone piatte del landscape energetico e piccoli passi in direzioni ripide e strette. <br>\n",
        "I cosidetti metodi del secondo ordine ottengono questo risultato calcolando o approssimando l'Hessiano e normalizzando il learning rate con la curvatura delal superficie nell'intorno del punto in cui ci si trova.  <br>\n",
        "Questi metodi tuttavia sono molto costosti dal punto di vista computazionale specie per modelli con un grande numero di parametri e quindi in pratica non sono applicabili. <br>\n",
        "Sono stati quindi introdotti numerosi metodi che consentono in modo computazionalemnte efficiente di adattare il learning rate seguendo il gradiente ed il momento secondo del gradiente stesso. <br>\n",
        "Metodi di questo tipo includono AdaGrad, AdaDelta, RMSProp e ADAM. \n",
        "\n",
        "* Esempio: RMSProp (Root-Mean-Square propagation)\n",
        "\n",
        "durante le iterazioni oltre a tenere in memoria il momento primo del gradiente attraverso una media mobile (running average), viene anche tenuta memoria del momento secondo. La regola di aggiornamento \u00e8 data da:\n",
        "\n",
        "$$\n",
        "\\mathbf{g}_t = \\nabla_w E(\\boldsymbol{w}) \\\\\n",
        "\\mathbf{s}_t =\\beta \\mathbf{s}_{t-1} +(1-\\beta)\\mathbf{g}_t^2 \\nonumber \\\\\n",
        "\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_t + \\eta_t { \\mathbf{g}_t \\over \\sqrt{\\mathbf{s}_t +\\epsilon}}, \\nonumber \\\\\n",
        "$$\n",
        "\n",
        "con $\\beta$ parametro che controlla il tempo di media del momento secondo (tipicamente $\\beta\\sim 0.9$), e \n",
        "$\\eta_t$ learning rate (tipicamente $\\sim 10^{-3}$ e $\\epsilon\\sim 10^{-8}$ una piccola costante che serve ad evitare divergenze.\n",
        "\n",
        "Si vede come il learning rate \u00e8 ridotto nelle direzioni in cui la norma del gradiente \u00e8 costantemente ampia. Ci\u00f2 accelera notevolmente la convergenza consentendoci di utilizzare un tasso di apprendimento pi\u00f9 ampio per direzioni piatte.\n",
        "\n",
        "* Esempio: ADAM (Adaptive Moment Estimation)\n",
        "\n",
        "Corregge le stime del primo e secondo momento per il bias dovuto al fatto che sono stime fatte con medie mobili:\n",
        "\n",
        "$$\n",
        "\\mathbf{g}_t = \\nabla_w E(\\boldsymbol{w}) \\\\\n",
        "\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\mathbf{g}_t \\nonumber \\\\\n",
        "\\mathbf{s}_t =\\beta_2 \\mathbf{s}_{t-1} +(1-\\beta_2)\\mathbf{g}_t^2 \\nonumber \\\\\n",
        "\\hat{\\mathbf{m}}_t={\\mathbf{m}_t \\over 1-\\beta_1} \\nonumber \\\\\n",
        "\\hat{\\mathbf{s}}_t ={\\mathbf{s}_t \\over1-\\beta_2} \\nonumber \\\\\n",
        "\\boldsymbol{w}_{t+1}=\\boldsymbol{w}_t + \\eta_t { \\hat{\\mathbf{m}}_t \\over \\sqrt{\\hat{\\mathbf{s}}_t +\\epsilon}}, \\nonumber \n",
        "$$\n",
        "\n",
        "con $\\beta_1$ e $\\beta_2$ che fissano le vite-medie del primo e secondo momento e sono tipicamente $0.9$ e $0.99$ rispettivamente. $\\eta$ e $\\epsilon$ sono gli stessi di RMSProp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Implementazione RMSProp e ADAM\n",
        "\n",
        "\n",
        "def rms_prop(grad, init, hyper_par, n_epochs=5000, noise_strength=0):\n",
        "    default_par = OrD([\n",
        "        (\"$\\eta$\", 10 ** -3), \n",
        "        (\"$\\beta$\", 0.9), \n",
        "        (\"$\\epsilon$\", 10 ** -8)\n",
        "        ])\n",
        "    eta, beta, epsilon = get_par(hyper_par, default_par)\n",
        "\n",
        "    params = np.array(init)\n",
        "    param_traj = np.zeros([n_epochs + 1, 2])\n",
        "    param_traj[0,] = init\n",
        "    grad_sq = 0\n",
        "    for j in range(n_epochs):\n",
        "        noise = noise_strength * np.random.randn(params.size)\n",
        "        g = np.array(grad(params)) + noise\n",
        "        grad_sq = beta * grad_sq + (1 - beta) * g * g\n",
        "        v = eta * np.divide(g, np.sqrt(grad_sq + epsilon))\n",
        "        params = params - v\n",
        "        param_traj[j + 1,] = params\n",
        "\n",
        "    legend = \"RMSprop \" + \", \".join(f\"{key}={hyper_par[key]}\" for key in hyper_par)\n",
        "    return (param_traj, legend)\n",
        "\n",
        "\n",
        "def adams(grad, init, hyper_par, n_epochs=5000, noise_strength=0):\n",
        "    default_par = OrD([\n",
        "            (\"$\\eta$\", 10 ** -3),\n",
        "            (\"$\\gamma$\", 0.9),\n",
        "            (\"$\\beta$\", 0.99),\n",
        "            (\"$\\epsilon$\", 10 ** -8),\n",
        "    ])\n",
        "    eta, gamma, beta, epsilon = get_par(hyper_par, default_par)\n",
        "\n",
        "    params = np.array(init)\n",
        "    param_traj = np.zeros([n_epochs + 1, 2])\n",
        "    param_traj[0,] = init\n",
        "    v = 0\n",
        "    grad_sq = 0\n",
        "    for j in range(n_epochs):\n",
        "        noise = noise_strength * np.random.randn(params.size)\n",
        "        g = np.array(grad(params)) + noise\n",
        "        v = gamma * v + (1 - gamma) * g\n",
        "        grad_sq = beta * grad_sq + (1 - beta) * g * g\n",
        "        v_hat = v / (1 - gamma)\n",
        "        grad_sq_hat = grad_sq / (1 - beta)\n",
        "        params = params - eta * np.divide(v_hat, np.sqrt(grad_sq_hat + epsilon))\n",
        "        param_traj[j + 1,] = params\n",
        "    legend = \"ADAM \" + \", \".join(f\"{key}={hyper_par[key]}\" for key in hyper_par)\n",
        "    return (param_traj, legend)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applicazione di RMSProp e ADAM\n",
        "\n",
        "\n",
        "Applichiamo quanto visto alla funzione di Beale: \n",
        "\n",
        "$$\n",
        "z = f(x,y) = (1.5-x+xy)^2+(2.25-x+xy^2)^2+(2.625-x+xy^3)^2.\n",
        "$$\n",
        "\n",
        "con minimo globale in $(x,y)=(3,0.5)$.\n",
        "\n",
        "**Cose si vuole testare:**\n",
        "\n",
        "* trovare il minimo con: GD, GD+momentum, RMSProp, ADAM, parteno da condizioni iniziali diverse\n",
        "* analizzare come cambiano le cose cambiando $\\eta$ e il numero di iterazioni\n",
        "* analizzare cosa si ottiene usando $N_{\\mathrm{steps}}=10^4$ e $\\eta=10^{-3}$ per ADAM/RMSProp e $\\eta=10^{-6}$ per GD e GD+momentum\n",
        "\n",
        "**Da fare come esercizio:**\n",
        "\n",
        "* cosa succede usando $\\eta=10^{-6}$ anche per ADAM/RMSProp \n",
        "* cosa succede inserendo del rumore nel gioco (ad esempio noise=10)?\n",
        "* giocate con i parametri degli algoritmi ADAM e RMSProp, cosa cambia?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.close()\n",
        "\n",
        "N1=10**4\n",
        "lr_l=10**-3\n",
        "lr_s=10**-6\n",
        "\n",
        "init1 = np.array([4, 3])\n",
        "fig1, ax1=contour_beales_function()\n",
        "\n",
        "grad = grad_beales_function\n",
        "\n",
        "#run_and_plot(ax, gradient, init, hyper_par, marker='o', n_epochs=100, opt=gd, arrow=False, noise=0):\n",
        "run_and_plot(ax1, grad, init1, {'$\\eta$': lr_s}, 'k', N1, gd, True)\n",
        "run_and_plot(ax1, grad, init1, {'$\\eta$': lr_s}, 'm', N1, gd_with_mom, True)\n",
        "run_and_plot(ax1, grad, init1, {'$\\eta$': lr_l}, 'b-.', N1, rms_prop, True)\n",
        "run_and_plot(ax1, grad, init1, {'$\\eta$': lr_l}, 'r', N1, adams, True)\n",
        "\n",
        "plt.legend(loc=2)\n",
        "\n",
        "init2=np.array([-1,4])\n",
        "\n",
        "N2=10**5\n",
        "run_and_plot(ax1, grad, init2, {'$\\eta$': lr_s}, 'k', N2, gd, True)\n",
        "run_and_plot(ax1, grad, init2, {'$\\eta$': lr_s}, 'm', N2, gd_with_mom, True)\n",
        "run_and_plot(ax1, grad, init2, {'$\\eta$': lr_l}, 'b-.', N2, rms_prop, True)\n",
        "run_and_plot(ax1, grad, init2, {'$\\eta$': lr_l}, 'r', N2, adams, True)\n",
        "\n",
        "init3=np.array([-2,-4])\n",
        "\n",
        "run_and_plot(ax1, grad, init3, {'$\\eta$': lr_s}, 'k', N2, gd, True)\n",
        "run_and_plot(ax1, grad, init3, {'$\\eta$': lr_s}, 'm', N2, gd_with_mom, True)\n",
        "run_and_plot(ax1, grad, init3, {'$\\eta$': lr_l}, 'b-.', N2, rms_prop, True)\n",
        "run_and_plot(ax1, grad, init3, {'$\\eta$': lr_l}, 'r', N2, adams, True)\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.close()\n",
        "\n",
        "N1=10**4\n",
        "lr_l=10**-3\n",
        "lr_s=10**-6\n",
        "grad = grad_beales_function\n",
        "\n",
        "init1 = np.array([4, 3])\n",
        "fig1, ax1=contour_beales_function()\n",
        "run_and_plot(ax1, grad, init1, {'$\\eta$': lr_l}, 'b-.', N1, rms_prop, True)\n",
        "plt.legend(loc=2)\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}